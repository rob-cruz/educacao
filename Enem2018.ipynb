{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1>Análise de dados Enem 2018</h1>\n",
    "\n",
    "<p>A intenção da análise que se segue é entender e retirar informações relevantes em relação aos participantes do <b>enem 2018</b></p>\n",
    "\n",
    "<p>Informações como a relação da média de notas com determinados grupos de candidatos, além de obter correlação entre as notas desses candidatos e as informações socioeconômicas e a contribuição dessas informações para o resultado obtido na avaliação</p>\n",
    "\n",
    "<p>Existe alguma relação entre a nota/resultado obtida e a região, estado, ou município dos estudantes?</p>\n",
    "\n",
    "<p>O PIB ou IDH de determinada região coopera para melhora ou piora da média do resultado naquela região?\n",
    "</p>\n",
    "\n",
    "<p>O Dataset utilizado foi extraído do site oficial de dados abertos do Governo Federal e pode ser baixados <a href #>neste link</a>, além disso, os dados préprocessados para o resultado desse estudo podem ser baixados <a href #>neste link</a></p>\n",
    "<h3>Definição de Objetivos - Primários e Secundários</h3>\n",
    "<h4>Objetivos Primários</h4>\n",
    "<ol>\n",
    "\t<li>Realizar analises de cada ano do Enem individualmente e Observar através de técnicas de regressão linear e machine learning as correlações entre as variáveis e o peso de cada variável no desempenho dos candidatos do enem</li>\n",
    "\t<li>Unir os datasets disponíveis entre os anos de 1998 e 2019 no site de dadis abertos do Governo Federal e analisar a evolução dos candidatos, além de identificar possíveis causas para tais desempenhos</li>\n",
    "\n",
    "</ol>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install plotly\n",
    "#!pip install cufflinks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "        <script type=\"text/javascript\">\n",
       "        window.PlotlyConfig = {MathJaxConfig: 'local'};\n",
       "        if (window.MathJax) {MathJax.Hub.Config({SVG: {font: \"STIX-Web\"}});}\n",
       "        if (typeof require !== 'undefined') {\n",
       "        require.undef(\"plotly\");\n",
       "        requirejs.config({\n",
       "            paths: {\n",
       "                'plotly': ['https://cdn.plot.ly/plotly-2.2.0.min']\n",
       "            }\n",
       "        });\n",
       "        require(['plotly'], function(Plotly) {\n",
       "            window._Plotly = Plotly;\n",
       "        });\n",
       "        }\n",
       "        </script>\n",
       "        "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Importando os módulos e pacotes necessários para plotar os gráficos de forma interativa\n",
    "import cufflinks as cf\n",
    "from plotly.offline import iplot\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "cf.go_offline()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>Os dados do dataset abaixo podem ser baixados em sua forma bruta em <a href>inep.com.br</a></h3>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>Reduzindo o dataset para otimizar a memória e melhor aproveitar o poder de processamento computacional</h2>\n",
    "\n",
    "Arquivos como os do enem são muito grandes (mais de 5 milhões de linhas e aproximadamente 3BG), o que pode dificultar a leitura dos arquivos .CSV, ao invés de realizar um filter nas colunas, tive de fazer esse filtro já na abertura e leitura do documento utilizando o comando \"usecols\". Sobre essa técnica é possível encontrar detalhes em um dos vídeos que compõe a série de análise de dados do canal <a href=\"https://www.youtube.com/watch?v=bC6Q9Uc80Xw&list=PL5TJqBvpXQv5N3iV68bGBkea0HjMk98lR&index=15\"> Programação Dinâmica </a> no youtube.\n",
    "\n",
    "Sendo assim, colunas que não serão utilizadas foram \"desprezadas\" deixando o dataset mais leve e possibilitando melhor processamento"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "##Filtrando as colunas que serão utilizadas na análise\n",
    "filtro_dados_enem = ['NU_INSCRICAO', 'NU_ANO', \n",
    "       'NO_MUNICIPIO_RESIDENCIA', 'SG_UF_RESIDENCIA',\n",
    "       'NU_IDADE', 'TP_SEXO', 'TP_ESTADO_CIVIL', 'TP_COR_RACA',\n",
    "       'TP_NACIONALIDADE', \n",
    "       'TP_ST_CONCLUSAO', 'TP_ANO_CONCLUIU', 'TP_ESCOLA', 'TP_ENSINO',\n",
    "       'IN_NOME_SOCIAL', \n",
    "       'TP_PRESENCA_CN', 'TP_PRESENCA_CH', 'TP_PRESENCA_LC',\n",
    "       'TP_PRESENCA_MT',  'NU_NOTA_CN', 'NU_NOTA_CH', 'NU_NOTA_LC',\n",
    "       'NU_NOTA_MT', 'NU_NOTA_COMP1',\n",
    "       'NU_NOTA_COMP2', 'NU_NOTA_COMP3', 'NU_NOTA_COMP4', 'NU_NOTA_COMP5',\n",
    "       'NU_NOTA_REDACAO', 'Q001', 'Q002', 'Q003', 'Q004', 'Q005', 'Q006',\n",
    "       'Q007', 'Q008', 'Q009', 'Q010', 'Q011', 'Q012', 'Q013', 'Q014',\n",
    "       'Q015', 'Q016', 'Q017', 'Q018', 'Q019', 'Q020', 'Q021', 'Q022',\n",
    "       'Q023', 'Q024', 'Q025']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "ename": "MemoryError",
     "evalue": "Unable to allocate 42.1 MiB for an array with shape (5513747,) and data type object",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mMemoryError\u001b[0m                               Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-4-10029564bad0>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[1;31m#das coluna filtradas que foram armazenadas na variável filtro_dados_enem\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 4\u001b[1;33m \u001b[0menem2018\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'MICRODADOS_ENEM_2018.csv'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msep\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m';'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mencoding\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'ISO 8859-1'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0musecols\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mfiltro_dados_enem\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\pandas\\io\\parsers.py\u001b[0m in \u001b[0;36mread_csv\u001b[1;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, squeeze, prefix, mangle_dupe_cols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, dialect, error_bad_lines, warn_bad_lines, delim_whitespace, low_memory, memory_map, float_precision)\u001b[0m\n\u001b[0;32m    684\u001b[0m     )\n\u001b[0;32m    685\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 686\u001b[1;33m     \u001b[1;32mreturn\u001b[0m \u001b[0m_read\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkwds\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    687\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    688\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\pandas\\io\\parsers.py\u001b[0m in \u001b[0;36m_read\u001b[1;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[0;32m    456\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    457\u001b[0m     \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 458\u001b[1;33m         \u001b[0mdata\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mparser\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mread\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnrows\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    459\u001b[0m     \u001b[1;32mfinally\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    460\u001b[0m         \u001b[0mparser\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mclose\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\pandas\\io\\parsers.py\u001b[0m in \u001b[0;36mread\u001b[1;34m(self, nrows)\u001b[0m\n\u001b[0;32m   1194\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mread\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnrows\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mNone\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1195\u001b[0m         \u001b[0mnrows\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0m_validate_integer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"nrows\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnrows\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1196\u001b[1;33m         \u001b[0mret\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_engine\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mread\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnrows\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1197\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1198\u001b[0m         \u001b[1;31m# May alter columns / col_dict\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\pandas\\io\\parsers.py\u001b[0m in \u001b[0;36mread\u001b[1;34m(self, nrows)\u001b[0m\n\u001b[0;32m   2153\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mread\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnrows\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mNone\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2154\u001b[0m         \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 2155\u001b[1;33m             \u001b[0mdata\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_reader\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mread\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnrows\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   2156\u001b[0m         \u001b[1;32mexcept\u001b[0m \u001b[0mStopIteration\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2157\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_first_chunk\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mpandas\\_libs\\parsers.pyx\u001b[0m in \u001b[0;36mpandas._libs.parsers.TextReader.read\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;32mpandas\\_libs\\parsers.pyx\u001b[0m in \u001b[0;36mpandas._libs.parsers.TextReader._read_low_memory\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;32mpandas\\_libs\\parsers.pyx\u001b[0m in \u001b[0;36mpandas._libs.parsers._concatenate_chunks\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;32m<__array_function__ internals>\u001b[0m in \u001b[0;36mconcatenate\u001b[1;34m(*args, **kwargs)\u001b[0m\n",
      "\u001b[1;31mMemoryError\u001b[0m: Unable to allocate 42.1 MiB for an array with shape (5513747,) and data type object"
     ]
    }
   ],
   "source": [
    "#Carregando normalmente o dataset, porém estou utilizando o comando \"usecols\" passando como parâmetro a lista \n",
    "#das coluna filtradas que foram armazenadas na variável filtro_dados_enem\n",
    "\n",
    "enem2018 = pd.read_csv('MICRODADOS_ENEM_2018.csv', sep=';', encoding='ISO 8859-1', usecols=filtro_dados_enem)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Visualizando as primeiras linhas do dataser apenas com as colunas carregadas redução de 137 para 60 colunas\n",
    "enem2018.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#memória utilizada no processamento do dataset\n",
    "enem2018.info(memory_usage='deep')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>Otimização dos tipos de dados</h3>\n",
    "<p>O dataframe ainda estava consumindo uma parte muito grande, tanto em memória, quanto em processamento, então foi necessário realizar a otimização dos tipos de dados de sua composição </p>\n",
    "<p> Utilizei como referência esse artigo do <a href=\"https://medium.com/@miltongamaneto/otimizando-o-armazenamento-da-mem%C3%B3ria-de-um-dataframe-pandas-ca8c4eb45699\">Milton Gama Netto</a> que mostra em detalhes o motivo e como realizar essa otimização. </p>\n",
    "<p>\"Em resumo o python superestima o tamanho dos dados inferindo por exemplo o tipo  <i>int64</i> para números inteiros, mesmo quando o dado em sí não precisa de 64bits reservado na memória. Agora já dá pra imaginar que uma tabela com mais de 5 milhões de instâncias com dados superestimados pode ficar imensa, mas sem necessidade.\" </p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Otimizando os tipos de dados do dataframe\n",
    "ints = enem2018.select_dtypes(include=['int64','int32','int16']).columns\n",
    "enem2018[ints] = enem2018[ints].apply(pd.to_numeric, downcast='integer')\n",
    "\n",
    "floats = enem2018.select_dtypes(include=['float']).columns\n",
    "enem2018[floats] = enem2018[floats].apply(pd.to_numeric, downcast='float')\n",
    "objects = enem2018.select_dtypes('object').columns\n",
    "enem2018[objects] = enem2018[objects].apply(lambda x: x.astype('category'))\n",
    "\n",
    "#Aqui já é possível verificar todas as colunas renomeadas e o tamanho da redução de \n",
    "\n",
    "#memória utilizada no processamento do dataset\n",
    "enem2018.info(memory_usage='deep')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>Renomeando as colunas</h3>\n",
    "<p>Os dados do Enem foram recentemente reformulados para obedecer a uma padronização, o que é ótimo, pois facilita no entendimento dos dados e no agrupamento, mas para uma visualização exploratória, acabam sendo pouco amigaveis, por exemplo no questionário socioeconômico, o nome das colunas estão como Q001, Q002, Q003 e assim segue. A ideia aqui é justamente traduzir esses códigos de (Q001 | Q002 | Q003) para (GRAU DE INSTRUCAO DO PAI|\n",
    "GRAU DE INSTRUCAO DA MAE | OCUPACAO DO PAI |) respectivamente com a ajuda do dicionario de dados que acompanha os arquivo do dataset</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Renomeando as colunas para que tenham rótulos mais amigaveis\n",
    "enem_2018 = enem2018.rename({\n",
    "'NU_INSCRICAO':'NSCRICAO', \n",
    "'NU_ANO':'ANO', \n",
    "'NO_MUNICIPIO_RESIDENCIA':'MUNICIPIO_RESIDENCIA', \n",
    "'SG_UF_RESIDENCIA':'UF_RESIDENCIA',\n",
    "'NU_IDADE':'IDADE', \n",
    "'TP_SEXO':'SEXO', \n",
    "'TP_ESTADO_CIVIL':'ESTADO_CIVIL', \n",
    "'TP_COR_RACA':'COR_RACA',\n",
    "'TP_NACIONALIDADE':'NACIONALIDADE', \n",
    "'NO_MUNICIPIO_NASCIMENTO':'MUNICIPIO_NASCIMENTO', \n",
    "'SG_UF_NASCIMENTO':'UF_NASCIMENTO',\n",
    "'TP_ST_CONCLUSAO':'T_CONCLUSAO', \n",
    "'TP_ANO_CONCLUIU':'ANO_CONCLUIU', \n",
    "'TP_ESCOLA':'TP_ESCOLA', \n",
    "'TP_ENSINO':'TP_ENSINO',\n",
    "'IN_TREINEIRO':'IN_TREINEIRO',  \n",
    "'NO_MUNICIPIO_ESC':'MUNICIPIO_ESC', \n",
    "'CO_UF_ESC':'COD_UF_ESC', \n",
    "'SG_UF_ESC': 'UF_ESC',\n",
    "'TP_DEPENDENCIA_ADM_ESC':'DEPENDENCIA_ADM_ESC', \n",
    "'TP_LOCALIZACAO_ESC':'LOCALIZACAO_ESC', \n",
    "'IN_NOME_SOCIAL':'NOME_SOCIAL', \n",
    "'NO_MUNICIPIO_PROVA':'MUNICIPIO_PROVA', \n",
    "'SG_UF_PROVA':'SG_UF_PROVA',\n",
    "'TP_PRESENCA_CN':'PRESENCA_CN', \n",
    "'TP_PRESENCA_CH':'PRESENCA_CH', \n",
    "'TP_PRESENCA_LC':'PRESENCA_LC',\n",
    "'TP_PRESENCA_MT':'PRESENCA_MT',  \n",
    "'NU_NOTA_CN':'NOTA_CN', \n",
    "'NU_NOTA_CH':'NOTA_CH', \n",
    "'NU_NOTA_LC':'NOTA_LC',\n",
    "'NU_NOTA_MT':'NOTA_MT', \n",
    "'TP_STATUS_REDACAO':'STATUS_REDACAO', \n",
    "'NU_NOTA_COMP1':'NOTA_COMP1',\n",
    "'NU_NOTA_COMP2':'NOTA_COMP2', \n",
    "'NU_NOTA_COMP3':'NOTA_COMP3', \n",
    "'NU_NOTA_COMP4':'NOTA_COMP4', \n",
    "'NU_NOTA_COMP5':'NOTA_COMP5',\n",
    "'NU_NOTA_REDACAO':'NOTA_REDACAO', \n",
    "'Q001': 'GRAU DE INSTRUCAO DO PAI',\n",
    "'Q002': 'GRAU DE INSTRUCAO DA MAE',\n",
    "'Q003': 'OCUPACAO DO PAI',\n",
    "'Q004': 'OCUPACAO DA MAE',\n",
    "'Q005': 'QUANTAS PESSOAS MORAM EM CASA',\n",
    "'Q006':\t'RENDA MENSAL FAMILIAR',\n",
    "'Q007': 'POSSUI EMPREGADO(A) DOMESTICO(A)',\n",
    "'Q008': 'POSSUI BANHEIRO',\n",
    "'Q009': 'POSSUI QUARTOS EM CASA',\n",
    "'Q010': 'POSSUI CARRO EM CASA',\n",
    "'Q011': 'POSSUI MOTOCICLETA',\n",
    "'Q012': 'POSSUI GELADEIRA',\n",
    "'Q013': 'POSSUI FREEZER (INDEPENDENTE DA GELADEIRA)',\n",
    "'Q014':\t'POSSUI MAQUINA DE LAVAR',\n",
    "'Q015': 'POSSUI MAQUINA DE SECAR ROUPAS',\n",
    "'Q016': 'POSSUI FORNO MICROONDAS',\n",
    "'Q017': 'POSSUI MAQUINA DE LAVAR LOUCAS',\n",
    "'Q018': 'POSSUI ASPIRADOR DE PO',\n",
    "'Q019': 'POSSUI TELEVISAO A CORES',\n",
    "'Q020': 'POSSUI APARELHO DVD',\n",
    "'Q021': 'POSSUI TV POR ASSINATURA',\n",
    "'Q022':\t'POSSUI TELEFONE CELULAR',\n",
    "'Q023': 'POSSUI TELEFONE FIXO',\n",
    "'Q024': 'POSSUI COMPUTADOR EM CASA',\n",
    "'Q025':'POSSUI INTERNET EM CASA'\n",
    "}, axis='columns')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Lendo as primeiras linhas do dataframe com as colunas já renomeadas\n",
    "enem_2018.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "enem_2018.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "enem_2018.to_csv('enem_2018.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "enem_2018 = pd.read_csv('enem_rename.csv', sep=';', encoding='ISO 8859-1')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "enem_2018"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
